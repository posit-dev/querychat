from __future__ import annotations

import copy
import os
import re
import sys
from pathlib import Path
from typing import TYPE_CHECKING, Literal, Optional, overload

import chatlas
import chevron
import sqlalchemy
from shiny import ui
from shiny.session import get_current_session

from ._querychat_module import ModServerResult, mod_server, mod_ui
from .datasource import DataFrameSource, DataSource, SQLAlchemySource

if TYPE_CHECKING:
    import pandas as pd
    from narwhals.stable.v1.typing import IntoFrame


class QueryChatBase:
    """
    Create a QueryChat instance.

    This is the main entry point for using querychat.
    """

    def __init__(
        self,
        data_source: IntoFrame | sqlalchemy.Engine,
        table_name: str,
        *,
        id: Optional[str] = None,
        greeting: Optional[str | Path] = None,
        client: Optional[str | chatlas.Chat] = None,
        data_description: Optional[str | Path] = None,
        extra_instructions: Optional[str | Path] = None,
        prompt_template: Optional[str | Path] = None,
    ):
        """
        Initialize QueryChat.

        Parameters
        ----------
        data_source
            Either a Narwhals-compatible data frame (e.g., Polars or Pandas) or a
            SQLAlchemy engine containing the table to query against.
        table_name
            If a data_source is a data frame, a name to use to refer to the table in
            SQL queries (usually the variable name of the data frame, but it doesn't
            have to be). If a data_source is a SQLAlchemy engine, the table_name is
            the name of the table in the database to query against.
        id
            An optional ID for the QueryChat module. If not provided, an ID will be
            generated based on the table_name.
        greeting
            A string in Markdown format, containing the initial message. If a
            pathlib.Path object is passed, querychat will read the contents of the
            path into a string with `.read_text()`. You can use
            `querychat.greeting()` to help generate a greeting from a querychat
            configuration. If no greeting is provided, one will be generated at the
            start of every new conversation.
        client
            A `chatlas.Chat` object or a string to be passed to
            `chatlas.ChatAuto()`'s `provider_model` parameter, describing the
            provider and model combination to use (e.g. `"openai/gpt-4.1"`,
            "anthropic/claude-sonnet-4-5", "google/gemini-2.5-flash". etc).

            If `client` is not provided, querychat consults the
            `QUERYCHAT_CLIENT` environment variable. If that is not set, it
            defaults to `"openai"`.
        data_description
            Description of the data in plain text or Markdown. If a pathlib.Path
            object is passed, querychat will read the contents of the path into a
            string with `.read_text()`.
        extra_instructions
            Additional instructions for the chat model. If a pathlib.Path object is
            passed, querychat will read the contents of the path into a string with
            `.read_text()`.
        prompt_template
            Path to or a string of a custom prompt file. If not provided, the default querychat
            template will be used. This should be a Markdown file that contains the
            system prompt template. The mustache template can use the following
            variables:
            - `{{db_engine}}`: The database engine used (e.g., "DuckDB")
            - `{{schema}}`: The schema of the data source, generated by
              `data_source.get_schema()`
            - `{{data_description}}`: The optional data description provided
            - `{{extra_instructions}}`: Any additional instructions provided

        Examples
        --------
        ```python
        from querychat import QueryChat

        qc = QueryChat(my_dataframe, "my_data")
        qc.app()
        ```

        """
        self.data_source = normalize_data_source(data_source, table_name)

        # Validate table name (must begin with letter, contain only letters, numbers, underscores)
        if not re.match(r"^[a-zA-Z][a-zA-Z0-9_]*$", table_name):
            raise ValueError(
                "Table name must begin with a letter and contain only letters, numbers, and underscores",
            )

        self.id = id or table_name

        self.client = normalize_client(client)

        if greeting is None:
            print(
                "Warning: No greeting provided; the LLM will be invoked at conversation start to generate one. "
                "For faster startup, lower cost, and determinism, please save a greeting and pass it to init().",
                "You can also use `querychat.greeting()` to help generate a greeting.",
                file=sys.stderr,
            )

        self.greeting = greeting.read_text() if isinstance(greeting, Path) else greeting

        self.system_prompt = get_system_prompt(
            self.data_source,
            data_description=data_description,
            extra_instructions=extra_instructions,
            prompt_template=prompt_template,
        )

        # Populated when ._server() gets called (in an active session)
        self._server_values: ModServerResult | None = None

    def sidebar(
        self,
        *,
        width: int = 400,
        height: str = "100%",
        **kwargs,
    ) -> ui.Sidebar:
        """
        Create a sidebar containing the querychat UI.

        Parameters
        ----------
        width
            Width of the sidebar in pixels.
        height
            Height of the sidebar.
        **kwargs
            Additional arguments passed to `shiny.ui.sidebar()`.

        Returns
        -------
        :
            A sidebar UI component.

        """
        return ui.sidebar(
            self.ui(),
            width=width,
            height=height,
            class_="querychat-sidebar",
            **kwargs,
        )

    def ui(self, **kwargs):
        """
        Create the UI for the querychat component.

        Parameters
        ----------
        **kwargs
            Additional arguments to pass to `shinychat.chat_ui()`.

        Returns
        -------
        :
            A UI component.

        """
        return mod_ui(self.id, **kwargs)

    def _server(self):
        """
        Initialize the server module.

        Note:
        ----
        This is a private method since it is called automatically in Express mode.

        """
        # Must be called within an active Shiny session
        session = get_current_session()
        if session is None:
            raise RuntimeError(
                "A Shiny session must be active in order to initialize QueryChat's server logic. "
                "If you're using Shiny Core, make sure to call .server() within your server function."
            )

        # No-op for Express' stub session (i.e., it's 1st run)
        if session.is_stub_session():
            return

        # Call the server module
        self._server_values = mod_server(
            self.id,
            data_source=self.data_source,
            system_prompt=self.system_prompt,
            greeting=self.greeting,
            client=self.client,
        )

        return

    def df(self) -> pd.DataFrame:
        """
        Reactively read the current filtered data frame that is in effect.

        Returns
        -------
        :
            The current filtered data frame as a pandas DataFrame. If no query
            has been set, this will return the unfiltered data frame from the
            data source.

        Raises
        ------
        RuntimeError
            If `.server()` has not been called yet.

        """
        vals = self._server_values
        if vals is None:
            raise RuntimeError("Must call .server() before accessing .df()")

        return vals.df()

    @overload
    def sql(self, query: None = None) -> str: ...

    @overload
    def sql(self, query: str) -> bool: ...

    def sql(self, query: Optional[str] = None) -> str | bool:
        """
        Reactively read (or set) the current SQL query that is in effect.

        Parameters
        ----------
        query
            If provided, sets the current SQL query to this value.

        Returns
        -------
        :
            If no `query` is provided, returns the current SQL query as a string
            (possibly `""` if no query has been set). If a `query` is provided,
            returns `True` if the query was changed to a new value, or `False`
            if it was the same as the current value.

        Raises
        ------
        RuntimeError
            If `.server()` has not been called yet.

        """
        vals = self._server_values
        if vals is None:
            raise RuntimeError("Must call .server() before accessing .sql()")

        if query is None:
            return vals.sql()
        else:
            return vals.sql.set(query)

    @overload
    def title(self, value: None = None) -> str | None: ...

    @overload
    def title(self, value: str) -> bool: ...

    def title(self, value: Optional[str] = None) -> str | None | bool:
        """
        Reactively read (or set) the current title that is in effect.

        The title is a short description of the current query that the LLM
        provides to us whenever it generates a new SQL query. It can be used as
        a status string for the data dashboard.

        Parameters
        ----------
        value
            If provided, sets the current title to this value.

        Returns
        -------
        :
            If no `value` is provided, returns the current title as a string, or
            `None` if no title has been set due to no SQL query being set. If a
            `value` is provided, sets the current title to this value and
            returns `True` if the title was changed to a new value, or `False`
            if it was the same as the current value.

        Raises
        ------
        RuntimeError
            If `.server()` has not been called yet.

        """
        vals = self._server_values
        if vals is None:
            raise RuntimeError("Must call .server() before accessing .title()")

        if value is None:
            return vals.title()
        else:
            return vals.title.set(value)

    def generate_greeting(self, *, echo: Literal["none", "text"] = "none"):
        """
        Generate a welcome greeting for the chat.

        By default, `QueryChat()` generates a greeting at the start of every new
        conversation, which is convenient for getting started and development,
        but also might add unnecessary latency and cost. Use this method to
        generate a greeting once and save it for reuse.

        Parameters
        ----------
        echo
            If `echo = "text"`, prints the greeting to standard output. If
            `echo = "none"` (default), does not print anything.

        Returns
        -------
        :
            The greeting string (in Markdown format).

        """
        client = copy.deepcopy(self.client)
        client.system_prompt = self.system_prompt
        client.set_turns([])
        prompt = "Please give me a friendly greeting. Include a few sample prompts in a two-level bulleted list."
        return str(client.chat(prompt, echo=echo))

    def set_system_prompt(
        self,
        data_source: DataSource,
        *,
        data_description: Optional[str | Path] = None,
        extra_instructions: Optional[str | Path] = None,
        categorical_threshold: int = 10,
        prompt_template: Optional[str | Path] = None,
    ) -> None:
        """
        Customize the system prompt.

        Control the logic behind how the system prompt is generated based on the
        data source's schema and optional additional context and instructions.

        Note
        ----
        This method is for parametrized system prompt generation only. To set a
        fully custom system prompt string, set the `system_prompt` attribute
        directly.

        Parameters
        ----------
        data_source
            A data source to generate schema information from
        data_description
            Optional description of the data, in plain text or Markdown format
        extra_instructions
            Optional additional instructions for the chat model, in plain text or
            Markdown format
        categorical_threshold
            Threshold for determining if a column is categorical based on number of
            unique values
        prompt_template
            Optional `Path` to or string of a custom prompt template. If not provided, the default
            querychat template will be used.

        """
        self.system_prompt = get_system_prompt(
            data_source,
            data_description=data_description,
            extra_instructions=extra_instructions,
            categorical_threshold=categorical_threshold,
            prompt_template=prompt_template,
        )

    def set_data_source(
        self, data_source: IntoFrame | sqlalchemy.Engine | DataSource, table_name: str
    ) -> None:
        """
        Set a new data source for the QueryChat object.

        Parameters
        ----------
        data_source
            The new data source to use.
        table_name
            If a data_source is a data frame, a name to use to refer to the table

        Returns
        -------
        :
            None

        """
        self.data_source = normalize_data_source(data_source, table_name)

    def set_client(self, client: str | chatlas.Chat) -> None:
        """
        Set a new chat client for the QueryChat object.

        Parameters
        ----------
        client
            A `chatlas.Chat` object or a string to be passed to
            `chatlas.ChatAuto()` describing the model to use (e.g.
            `"openai/gpt-4.1"`).

        Returns
        -------
        :
            None

        """
        self.client = normalize_client(client)


class QueryChat(QueryChatBase):
    def server(self):
        """
        Initialize Shiny server logic.

        This method is intended for use in Shiny Code mode, where the user must
        explicitly call `.server()` within the Shiny server function. In Shiny
        Express mode, you can use `querychat.express.QueryChat` instead
        of `querychat.QueryChat`, which calls `.server()` automatically.

        Examples
        --------
        ```python
        from shiny import App, render, ui
        from querychat import QueryChat

        qc = QueryChat(my_dataframe, "my_data")

        app_ui = ui.page_fluid(
            qc.sidebar(),
            ui.output_data_frame("data_table"),
        )


        def server(input, output, session):
            qc.server()

            @render.data_frame
            def data_table():
                return qc.df()


        app = App(app_ui, server)
        ```

        Returns
        -------
        :
            None

        """
        return self._server()


class QueryChatExpress(QueryChatBase):
    """
    Use QueryChat with Shiny Express mode.

    This class makes it easy to use querychat within Shiny Express apps --
    it automatically calls `.server()` during initialization, so you don't
    have to do it manually.

    Examples
    --------
    ```python
    from shiny.express import render, ui
    from querychat.express import QueryChat

    qc = QueryChat(my_dataframe, "my_data")

    qc.sidebar()


    @render.data_frame
    def data_table():
        return qc.df()
    ```

    """

    def __init__(
        self,
        data_source: IntoFrame | sqlalchemy.Engine,
        table_name: str,
        *,
        id: Optional[str] = None,
        greeting: Optional[str | Path] = None,
        client: Optional[str | chatlas.Chat] = None,
        data_description: Optional[str | Path] = None,
        extra_instructions: Optional[str | Path] = None,
        prompt_template: Optional[str | Path] = None,
    ):
        super().__init__(
            data_source,
            table_name,
            id=id,
            greeting=greeting,
            client=client,
            data_description=data_description,
            extra_instructions=extra_instructions,
            prompt_template=prompt_template,
        )
        self._server()


def normalize_data_source(
    data_source: IntoFrame | sqlalchemy.Engine | DataSource,
    table_name: str,
) -> DataSource:
    if isinstance(data_source, DataSource):
        return data_source
    if isinstance(data_source, sqlalchemy.Engine):
        return SQLAlchemySource(data_source, table_name)
    return DataFrameSource(data_source, table_name)


def normalize_client(client: str | chatlas.Chat | None) -> chatlas.Chat:
    if client is None:
        client = os.getenv("QUERYCHAT_CLIENT", None)

    if client is None:
        client = "openai"

    if isinstance(client, chatlas.Chat):
        return client

    return chatlas.ChatAuto(provider_model=client)


def get_system_prompt(
    data_source: DataSource,
    *,
    data_description: Optional[str | Path] = None,
    extra_instructions: Optional[str | Path] = None,
    categorical_threshold: int = 10,
    prompt_template: Optional[str | Path] = None,
) -> str:
    # Read the prompt file
    if prompt_template is None:
        # Default to the prompt file in the same directory as this module
        # This allows for easy customization by placing a different prompt.md file there
        prompt_template = Path(__file__).parent / "prompts" / "prompt.md"
    prompt_str = (
        prompt_template.read_text()
        if isinstance(prompt_template, Path)
        else prompt_template
    )

    data_description_str = (
        data_description.read_text()
        if isinstance(data_description, Path)
        else data_description
    )

    extra_instructions_str = (
        extra_instructions.read_text()
        if isinstance(extra_instructions, Path)
        else extra_instructions
    )

    is_duck_db = data_source.get_db_type().lower() == "duckdb"

    return chevron.render(
        prompt_str,
        {
            "db_type": data_source.get_db_type(),
            "is_duck_db": is_duck_db,
            "schema": data_source.get_schema(
                categorical_threshold=categorical_threshold,
            ),
            "data_description": data_description_str,
            "extra_instructions": extra_instructions_str,
        },
    )
